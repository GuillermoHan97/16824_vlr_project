<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>Design Drafts Transfer</title>
<meta name="description" content="16824 Visual Learning Recognition Group Project">
<meta name="author" content="Yu Han">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="DPS/css/project.css">
</head>
	<script> 
	function coming_soon()
	{
		alert("We are cleaning up our code to make it more simple and readable");
	}
	</script> 

<body>
<div id="main">
  
	<div class="content"><br>
		<div class="title">
			<p class="banner"align="center">16824 Visual Learning Recognition Group Project</p>
			<h1>Social interaction Analysis on <br/> Egocentric Videos</h1>
		</div>
		<div class="authors" style="width:95%">
			<div class='author'>
				<A style="text-decoration: none">Yu Han</A>
		   </div>
			<div class='author'>
				 <A style="text-decoration: none">Xuhua Huang</A>
		    </div>
			<div class='author'>
				<A style="text-decoration: none">Xiaochen Han</A>
		   </div>			
		</div>
		
		<div class="motivation_sec">
			<h2>Motivation</h2>
			<div class='desp'>
				<p style="text-align:justify">
					As the heat continues to grow for the "Metaverse" concept in both industry and academia, more and more people have started to focus on first-person visual perception. Egocentric videos, which are captured by a wearable camera in
first-person perspective, have become the essential media for this new task. However, though egocentric videos can be viewed as a type of videos, they are more challenging compared with classical
videos because they usually involve rapid scene change, object distortion and limited visual range. Thanks to the recently published massive-scale ego-video dataset Ego4D [1], large amount of diverse data with high-quality annotation become accessible. Moreover, as an extension to existing visual learning tasks such as video analysis, egocentric video analysis is playing an significant role in improving human-human or human-computer interaction. Therefore, we decide to set egocentric videos analysis as the main direction of our project, with a particular emphasis on social interaction track.
				</p>
			</div>
			<div class="image" style="padding: 2em 0 0.5em 0">
				<table border="0" width='100%' style="FONT-SIZE:15" >
				 <tr align="center">
					<td width="80%"><img src="DPS/figures/Ego4d.jpg" alt="" width="80%" ></td>
				 </tr>
				 </table>							 
		  		<p style="text-align: justify">Figure 1. Example of egocentric videos of daily life activity [1].</p>
			  </div>
		</div>

		<div class="prior_sec">
			<h2>Related Work</h2>
			<div class='desp'>
				<p style="text-align:justify">
				abc
				</p>
			</div>
		</div>

		<div class="our_sec">
			<h2>Our idea</h2>
			<div class='desp'>
				<p style="text-align:justify">
				<h3>Data Imbalance Problem</h3>
				 </p>
				 By analyzing the data, we find out that we are facing an imbalance problem.
				 Below is the figure of percentage of frames that has looking at me annotations. 
				 Over 95% of data is not looking at me

				 <div class="image" style="padding: 2em 0 0.5em 0">
					<table border="0" width='100%' style="FONT-SIZE:15" >
					 <tr align="center">
						<td width="80%"><img src="DPS/figures/imbalance.png" alt="" width="40%" ></td>
					 </tr>
					 </table>							 
					  <p style="text-align: justify">Figure 2. Percentage of frames that has looking at me annotations [1].</p>
				  </div>

				  Instead of the whole big dataset, we actually select a subset for training and evaluating.

				  First we try to apply the weighted loss. We implement it in 2ways:
				  <br>
					1. Find the best fixed weight
					<br>
					2. Caculate the weight based on the training data batch

					<br>
					Also We design different evaluation metrics.
					Since though baseline has high accuracy, it predicts all data as not looking at me

					These metrics can balance the accuracy contributed from different classes.
					<br>
					Also we tried some basic data augmentation, mixup as well as dropout.
				  	<br>
					Different from the original mixup, we will mix with the looking at me class more frequently.
					And we adjust the loss weight based on the mixup result.

			</div>
		</div>

		<div class="experiment_sec">
			<h2>Experimental Results</h2>
			<div class='desp'>
				<p style="text-align:justify">
					<h3>Weighted Loss Result</h3>

					Both fixed weight Or dynamic weight can improve the performance. The weight 0.05 is actually very close to the percentage of looking at me annotation in the data.

					<h3>Data Augmentation and Dropout Result</h3>


					To sum up, the data is actually very noisy.
					<br>
					Since the quality of image is not good.
					And also the motion is big since it's ego video.

					The bounding box for faces are very unstable as in the below figure.

					<div class="image" style="padding: 2em 0 0.5em 0">
						<table border="0" width='100%' style="FONT-SIZE:15" >
						 <tr align="center">
							<td width="80%"><img src="DPS/figures/noisy.jpg" alt="" width="40%" ></td>
						 </tr>
						 </table>							 
						  <p style="text-align: justify">Figure 3. Noisy data with unstable face bounding box.</p>
					  </div>

					So the improvement is not that big.
					<br>
					For dropout, it is actually very unstable for the imbalance task

					
				 </p>
			</div>
		</div>


		<div class="Conclusion_sec">
			<h2>Conclusion</h2>
			<div class='desp'>
				<p style="text-align:justify">
				abc
				 </p>
			</div>
		</div>

		<div class="future_sec">
			<h2>Future Direction</h2>
			<div class='desp'>
				<p style="text-align:justify">
				abc
				</p>
			</div>
		</div>
		
		<div class="reference_sec">
		<h2>Reference</h2>
		  <div class="bib" style="text-align: justify">
		    <p>[1] K. Grauman, etc. Ego4d: Around the World in 3000 Hours of Egocentric Video. CoRR 2021.</p>
		  </div>
		</div>
		
		<br></br> 
	
</div>
</div>
</body>
</html>
