<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>Design Drafts Transfer</title>
<meta name="description" content="16824 Visual Learning Recognition Group Project">
<meta name="author" content="Yu Han">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="DPS/css/project.css">
</head>
    <script>
    function coming_soon()
    {
        alert("We are cleaning up our code to make it more simple and readable");
    }
    </script>

<body>
<div id="main">

    <div class="content"><br>
        <div class="title">
            <p class="banner"align="center">16824 Visual Learning Recognition Group Project</p>
            <h1>Social interaction Analysis on <br/> Egocentric Videos</h1>
        </div>
        <div class="authors" style="width:95%">
            <div class='author'>
                <A style="text-decoration: none">Yu Han</A>
           </div>
            <div class='author'>
                 <A style="text-decoration: none">Xuhua Huang</A>
            </div>
            <div class='author'>
                <A style="text-decoration: none">Xiaochen Han</A>
           </div>
        </div>

        <div class="motivation_sec">
            <h2>Motivation</h2>
            <div class='desp'>
                <p style="text-align:justify">
                    As the heat continues to grow for the "Metaverse" concept in both industry and academia, more and more people have started to focus on first-person visual perception. Egocentric videos, which are captured by a wearable camera in
first-person perspective, have become the essential media for this new task. However, though egocentric videos can be viewed as a type of videos, they are more challenging compared with classical
videos because they usually involve rapid scene change, object distortion and limited visual range. Thanks to the recently published massive-scale ego-video dataset Ego4D [1], large amount of diverse data with high-quality annotation become accessible. Moreover, as an extension to existing visual learning tasks such as video analysis, egocentric video analysis is playing an significant role in improving human-human or human-computer interaction. Therefore, we decide to set egocentric videos analysis as the main direction of our project, with a particular emphasis on social interaction track.
                </p>
            </div>
            <div class="image" style="padding: 2em 0 0.5em 0">
                <table border="0" width='100%' style="FONT-SIZE:15" >
                 <tr align="center">
                    <td width="80%"><img src="DPS/figures/Ego4d.jpg" alt="" width="80%" ></td>
                 </tr>
                 </table>
                  <p style="text-align: justify">Figure 1. Example of egocentric videos of daily life activity [1].</p>
              </div>
        </div>

        <div class="prior_sec">
            <h2>Related Work</h2>
            <div class='desp'>
                <p style="text-align:justify">
                abc
                </p>
            </div>
        </div>

        <div class="our_sec">
            <h2>Our Ideas</h2>
            <div class='desp'>
                <p style="text-align:justify">
                <h3>Data Imbalance Problem</h3>
                 </p>
                 By analyzing the data, we find out that we are facing an imbalance problem.
                 Below is the figure of percentage of frames that has looking at me annotations.
                 Over 95% of data is not looking at me

                 <div class="image" style="padding: 2em 0 0.5em 0">
                    <table border="0" width='100%' style="FONT-SIZE:15" >
                     <tr align="center">
                        <td width="80%"><img src="DPS/figures/imbalance.png" alt="" width="40%" ></td>
                     </tr>
                     </table>
                      <p style="text-align: justify">Figure 2. Percentage of frames that has looking at me annotations [1].</p>
                  </div>

                  Instead of the whole big dataset, we actually select a subset for training and evaluating.

                  First we try to apply the weighted loss. We implement it in 2ways:
                  <br>
                    1. Find the best fixed weight
                    <br>
                    2. Caculate the weight based on the training data batch

                    <br>
                    Also We design different evaluation metrics.
                    Since though baseline has high accuracy, it predicts all data as not looking at me

                    These metrics can balance the accuracy contributed from different classes.
                    <br>
                    Also we tried some basic data augmentation, mixup as well as dropout.
                      <br>
                    Different from the original mixup, we will mix with the looking at me class more frequently.
                    And we adjust the loss weight based on the mixup result.

                <p style="text-align:justify">
                <h3>More Reasonable Metrics</h3>
                 </p>
                 According to our anaysis on this dataset, we notice that it is seriously imbalanced so even if the Baseline
                 predicts ALL data as "not looking at me", it still achieves very high accuracy because most of the GT annotations are "not looking at me".
                 Therefore, we proposed 4 more reasonable metrics, which can be viewed as "weighted accuracy" aiming to balance the accuracy contributed from different classes.
                 All these metrics are shown in Figure 3.
                 <div class="image" style="padding: 2em 0 0.5em 0">
                    <table border="0" width='100%' style="FONT-SIZE:15" >
                     <tr align="center">
                        <td width="80%"><img src="DPS/figures/metrics.png" alt="" width="40%" ></td>
                     </tr>
                     </table>
                      <p style="text-align: justify">Figure 2. Percentage of frames that has looking at me annotations [1].</p>
                  </div>

                <p style="text-align:justify">
                <h3>Transformer</h3>
                Transformer is a novel deep learning architecture that adopts the self-attention mechanism, and trying to learn
                an adaptive weighting system to capture the significance of each part of the input data. In our task, it is especially
                helpful to focus on the gaze located around eye region, which can provide a useful clue for our model to learn "looking at me".
                Therefore, we decide to adapt a recently proposed technique named Vision Transformer [2] to our task.

                As a brief introduction, ViT will divide the input image into multiple patches and then for each patch it will extract a feature vector,
                and pass through the Transformer architecture, then followed by a MLP head to generate classification results. In our setting, the input image
                will become the face region of different persons interacting with camera wearer, and the output of MLP head will be a binary boolean label deciding
                whether the person is looking at me or not. Our experiment results proved that Transformer is good at attending to important regions for our task.
                 </p>
                 <div class="image" style="padding: 2em 0 0.5em 0">
                    <table border="0" width='100%' style="FONT-SIZE:15" >
                     <tr align="center">
                        <td width="80%"><img src="DPS/figures/vit.png" alt="" width="80%" ></td>
                     </tr>
                     </table>
                      <p style="text-align: justify">Figure 3. Vision Transformer architecture [2].</p>
                  </div>
            </div>
        </div>

        <div class="experiment_sec">
            <h2>Experimental Results</h2>
            <div class='desp'>
                <p style="text-align:justify">
                    <h3>Weighted Loss Result</h3>

                    Both fixed weight Or dynamic weight can improve the performance. The weight 0.05 is actually very close to the percentage of looking at me annotation in the data.

                    <h3>Data Augmentation and Dropout Result</h3>


                    To sum up, the data is actually very noisy.
                    <br>
                    Since the quality of image is not good.
                    And also the motion is big since it's ego video.

                    The bounding box for faces are very unstable as in the below figure.

                    <div class="image" style="padding: 2em 0 0.5em 0">
                        <table border="0" width='100%' style="FONT-SIZE:15" >
                         <tr align="center">
                            <td width="80%"><img src="DPS/figures/noisy.jpg" alt="" width="40%" ></td>
                         </tr>
                         </table>
                          <p style="text-align: justify">Figure 4. Noisy data with unstable face bounding box.</p>
                      </div>

                    So the improvement is not that big.
                    <br>
                    For dropout, it is actually very unstable for the imbalance task

                    <h3>Transformer Result</h3>
                    <div class="image" style="padding: 2em 0 0.5em 0">
                        <table border="0" width='100%' style="FONT-SIZE:15" >
                         <tr align="center">
                            <td width="80%"><img src="DPS/figures/trans_res.png" alt="" width="100%" ></td>
                         </tr>
                         </table>
                          <p style="text-align: justify">Figure 5. Transformer results.</p>
                  </div>
                  We first tried using transformer without pretrained weights, as we can see, this method had a worse performance in some metrics compared with Baseline, probably because Transformer is extremely hungry for high-quality data,
                  while in this dataset, we have limited access to high-quality and well-balanced data.
                  We also tried to finetune a Transformer model pretrained on other dataset, and as we can see, it can do better prediction because it is able to capture critical region in the image.

                 </p>
            </div>
        </div>


        <div class="Conclusion_sec">
            <h2>Conclusion</h2>
            <div class='desp'>
                <p style="text-align:justify">
                abc
                 </p>
            </div>
        </div>

        <div class="future_sec">
            <h2>Future Direction</h2>
            <div class='desp'>
                <p style="text-align:justify">
                abc
                </p>
            </div>
        </div>

        <div class="reference_sec">
        <h2>Reference</h2>
          <div class="bib" style="text-align: justify">
            <p>[1] K. Grauman, et al. "Ego4d: Around the World in 3000 Hours of Egocentric Video." CVPR 2022.</p>
            <p>[2] Alexey Dosovitskiy, et al. "An Image is worth 16x16 words: Transformers  for  image recognition at scale" ICLR 2021.</p>
          </div>
        </div>

        <br></br>

</div>
</div>
</body>
</html>
