<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>Design Drafts Transfer</title>
<meta name="description" content="16824 Visual Learning Recognition Group Project">
<meta name="author" content="Yu Han, Xuhua Huang, Xiaochen Han">

<link rel="stylesheet" type="text/css" href="DPS/css/project.css">

<body>
<div id="main">

    <div class="content"><br>
        <div class="title">
            <p class="banner"align="center">16824 Visual Learning Recognition Group Project</p>
            
            <h1>Social interaction Analysis on <br/> Egocentric Videos</h1>
        </div>
        <div class="authors" style="width:95%">
            <div class='author'>
                <A style="text-decoration: none">Yu Han</A>
           </div>
            <div class='author'>
                 <A style="text-decoration: none">Xuhua Huang</A>
            </div>
            <div class='author'>
                <A style="text-decoration: none">Xiaochen Han</A>
           </div>
        </div>

        <div class="resource_sec">
            <h2>Resource</h2>
            <div class='desp'>
                <p style="text-align:justify">
                    <A href="https://guillermohan97.github.io/16824_vlr_project/" style="text-decoration: none">Project Page (https://guillermohan97.github.io/16824_vlr_project/)</A>
                    <br>
                    <A href="https://github.com/GuillermoHan97/16824_vlr_project" style="text-decoration: none">Github repo (https://github.com/GuillermoHan97/16824_vlr_project)</A>
                </p>
        </div>

        <div class="motivation_sec">
            <h2>Motivation</h2>
            <div class='desp'>
                <p style="text-align:justify">
                    As the heat continues to grow for the "Metaverse" concept in both industry and academia, more and more people have started to focus on first-person visual perception. Egocentric videos, which are captured by a wearable camera in
first-person perspective, have become the essential media for this new task. However, though egocentric videos can be viewed as a type of videos, they are more challenging compared with classical
videos because they usually involve rapid scene change, object distortion and limited visual range. Thanks to the recently published massive-scale ego-video dataset Ego4D [1], large amount of diverse data with high-quality annotation become accessible. Moreover, as an extension to existing visual learning tasks such as video analysis, egocentric video analysis is playing an significant role in improving human-human or human-computer interaction. Therefore, we decide to set egocentric videos analysis as the main direction of our project, with a particular emphasis on social interaction track.
                </p>
            </div>
            <div class="image" style="padding: 2em 0 0.5em 0">
                <table border="0" width='100%' style="FONT-SIZE:15" >
                 <tr align="center">
                    <td width="80%"><img src="DPS/figures/Ego4d.jpg" alt="" width="80%" ></td>
                 </tr>
                 </table>
                  <p style="text-align: justify">Figure 1. Example of egocentric videos of daily life activity [1].</p>
            </div>
        </div>

        <div class="prior_sec">
            <h2>Related Work</h2>
            <div class='desp'>
                <p style="text-align:justify">
                    Video Analysis. As one of the most essential and important tasks in visual learning, video analysis distinguishes itself from previous image analysis task by its additional temporal dimension. 
                    This encourages many state-of-the-art methods in this field to put most efforts into improving temporal coherence without loss of 2D visual signal. 
                    For example,[3] proposes a novel architecture with a slow pathway to encode 2D visual information and a fast pathway to encode temporal information.
                    [9] also introduces a two-stream 3D convolution network that is based on 2D convolution inflation, in order to capture both visual and temporal features.
                    [4] presents non-local operations that are good at capturing long-range dependencies. All these methods from classical video analysis have turned out to be the building blocks for egocentric video analysis. 

                    Egocentric Video Analysis. Given the temporal nature of videos, many state-of-the-art methods targeted for egocentric videos such as [5,6,7] try to utilize attention mechanism [8] to capture the relationship across frames. 
                    The massive-scale egocentric video dataset Ego4D [1] also comes with a suite of benchmarks and baselines spanning many essential tasks of egocentric perception. 
                    More specifically, for the social interaction track that we aim to focus on, they have established a baseline with a ResNet-18[10] followed by a Bidirectional LSTM[11]. 
                </p>
            </div>
            <h2>Baseline</h2>
            <div class='desp'>
                <p style="text-align:justify">
                We adopted one of the most famous Gaze360[12] in this area as our baseline model. 
                Taking 7 frames (1 key current frame, 3 previous frames and 3 latter frames), 
                the straight forward model consists of a CNN structure (ResNet-18) to extract the image feature and a Bi-LSTM structure to align the temporal features.
                We modified the output layer to a FC layer for classification task.

                </p>
            </div>
            <div class="image" style="padding: 2em 0 0.5em 0">
                <table border="0" width='100%' style="FONT-SIZE:15" >
                 <tr align="center">
                    <td width="80%"><img src="DPS/figures/baseline.png" alt="" width="80%" ></td>
                 </tr>
                 </table>
                  <p style="text-align: justify">Figure 2. Gaze360[12] baseline.</p>
            </div>
        </div>

        <div class="our_sec">
            <h2>Our Ideas</h2>
            <div class='desp'>
                <p style="text-align:justify">
                <h3>Data Imbalance Problem</h3>
                 </p>
                 By analyzing the data, we find out that we are facing an imbalance problem.
                 Below is the figure of percentage of frames that has looking at me annotations.
                 Over 95% of data is not looking at me

                 <div class="image" style="padding: 2em 0 0.5em 0">
                    <table border="0" width='100%' style="FONT-SIZE:15" >
                     <tr align="center">
                        <td width="80%"><img src="DPS/figures/imbalance.png" alt="" width="40%" ></td>
                     </tr>
                     </table>
                      <p style="text-align: justify">Figure 3. Percentage of frames that has looking at me annotations [1].</p>
                  </div>

                  Instead of the whole big dataset, we actually select a subset for training and evaluating.

                  First we try to apply the weighted loss. We implement it in 2ways:
                  <br>
                    1. Find the best fixed weight
                    <br>
                    2. Caculate the weight based on the training data batch

                    <br>
                    Also We design different evaluation metrics.
                    Since though baseline has high accuracy, it predicts all data as not looking at me

                    These metrics can balance the accuracy contributed from different classes.
                    <br>
                    Also we tried some basic data augmentation, mixup as well as dropout.
                      <br>
                    Different from the original mixup, we will mix with the looking at me class more frequently.
                    And we adjust the loss weight based on the mixup result.

                <p style="text-align:justify">
                <h3>More Reasonable Metrics</h3>
                 </p>
                 According to our anaysis on this dataset, we notice that it is seriously imbalanced so even if the Baseline
                 predicts ALL data as "not looking at me", it still achieves very high accuracy because most of the GT annotations are "not looking at me".
                 Therefore, we proposed 4 more reasonable metrics, which can be viewed as "weighted accuracy" aiming to balance the accuracy contributed from different classes.
                 All these metrics are shown in Figure 3.
                 <div class="image" style="padding: 2em 0 0.5em 0">
                    <table border="0" width='100%' style="FONT-SIZE:15" >
                     <tr align="center">
                        <td width="80%"><img src="DPS/figures/metrics.png" alt="" width="40%" ></td>
                     </tr>
                     </table>
                      <p style="text-align: justify">Figure 4. Percentage of frames that has looking at me annotations [1].</p>
                  </div>

                <p style="text-align:justify">
                <h3>Transformer</h3>
                Transformer is a novel deep learning architecture that adopts the self-attention mechanism, and trying to learn
                an adaptive weighting system to capture the significance of each part of the input data. In our task, it is especially
                helpful to focus on the gaze located around eye region, which can provide a useful clue for our model to learn "looking at me".
                Therefore, we decide to adapt a recently proposed technique named Vision Transformer [2] to our task.

                As a brief introduction, ViT will divide the input image into multiple patches and then for each patch it will extract a feature vector,
                and pass through the Transformer architecture, then followed by a MLP head to generate classification results. In our setting, the input image
                will become the face region of different persons interacting with camera wearer, and the output of MLP head will be a binary boolean label deciding
                whether the person is looking at me or not. Our experiment results proved that Transformer is good at attending to important regions for our task.
                 </p>
                 <div class="image" style="padding: 2em 0 0.5em 0">
                    <table border="0" width='100%' style="FONT-SIZE:15" >
                     <tr align="center">
                        <td width="80%"><img src="DPS/figures/vit.png" alt="" width="80%" ></td>
                     </tr>
                     </table>
                      <p style="text-align: justify">Figure 5. Vision Transformer architecture [2].</p>
                  </div>
            </div>
        </div>

        <div class="experiment_sec">
            <h2>Experimental Results</h2>
            <div class='desp'>
                <p style="text-align:justify">
                    <h3>Weighted Loss Result</h3>

                    Both fixed weight Or dynamic weight can improve the performance. The weight 0.05 is actually very close to the percentage of looking at me annotation in the data.

                    <div class="image" style="padding: 2em 0 0.5em 0">
                        <table border="0" width='100%' style="FONT-SIZE:15" >
                         <tr align="center">
                            <td width="80%"><img src="DPS/figures/weight_loss_res.png" alt="" width="100%" ></td>
                         </tr>
                         </table>
                          <p style="text-align: justify">Figure 6. Weighted loss results.</p>
                    </div>

                    <h3>Data Augmentation and Dropout Result</h3>

                    <div class="image" style="padding: 2em 0 0.5em 0">
                        <table border="0" width='100%' style="FONT-SIZE:15" >
                         <tr align="center">
                            <td width="80%"><img src="DPS/figures/data_aug_res.png" alt="" width="100%" ></td>
                         </tr>
                         </table>
                          <p style="text-align: justify">Figure 7. Data augmentation results.</p>
                    </div>


                    To sum up, the data is actually very noisy.
                    <br>
                    Since the quality of image is not good.
                    And also the motion is big since it's ego video.

                    The bounding box for faces are very unstable as in the below figure.

                    <div class="image" style="padding: 2em 0 0.5em 0">
                        <table border="0" width='100%' style="FONT-SIZE:15" >
                         <tr align="center">
                            <td width="80%"><img src="DPS/figures/noisy.jpg" alt="" width="40%" ></td>
                         </tr>
                         </table>
                          <p style="text-align: justify">Figure 8. Noisy data with unstable face bounding box.</p>
                      </div>

                    So the improvement is not that big.
                    <br>
                    For dropout, it is actually very unstable for the imbalance task

                    <h3>Transformer Result</h3>
                    <div class="image" style="padding: 2em 0 0.5em 0">
                        <table border="0" width='100%' style="FONT-SIZE:15" >
                         <tr align="center">
                            <td width="80%"><img src="DPS/figures/trans_res.png" alt="" width="100%" ></td>
                         </tr>
                         </table>
                          <p style="text-align: justify">Figure 9. Transformer results.</p>
                    </div>
                  We first tried using transformer without pretrained weights, as we can see, this method had a worse performance in some metrics compared with Baseline, probably because Transformer is extremely hungry for high-quality data,
                  while in this dataset, we have limited access to high-quality and well-balanced data.
                  We also tried to finetune a Transformer model pretrained on other dataset, and as we can see, it can do better prediction because it is able to capture critical region in the image.

                 </p>
            </div>
        </div>


        <div class="Conclusion_sec">
            <h2>Conclusion</h2>
            <div class='desp'>
                <p style="text-align:justify">
                We selected the recently published dataset Ego4d which consists of egocentric videos and focused on the social interaction track, specifically the task of judging whether there are people looking at the camera at the current frame. 
                Based on the Gaze360 baseline, we first analyzed the dataset itself, noticed the problem of extreme imbalance thus tried weighted loss and more reasonable metrics. Data augmentation was also performed to alleviate the data imbalance problem.
                Experimental results show that by adding weighted loss, using new metrics and performing data augmentation, there are significant performance improvements over the baseline. 

                With the idea that for social interaction analysis, some regions such as eyes are more important than others so it would be a good idea to introduce attention mechanism. We used ViT structure in our model, which is supposed to focus on the eye 
                area of each frame. The experimental results proved that our proposed model with ViT structure outperformed the baseline with a large gap no matter whether with or without pretraining.
                </p>
            </div>
        </div>

        <div class="future_sec">
            <h2>Future Direction</h2>
            <div class='desp'>
                <p style="text-align:justify">
                Currently, the input for our Transformer structure is only the single key frame itself. As our goal is to detect eye contaction in video frames, sequential information is essential. 
                Compared with baseline which takes as input 7 frames nearby, our transformer is good at focusing more on critical spatial information. It would be a promising direction to combine 
                with LSTM-like mechanism to encode more temporal information.
                </p>
            </div>
        </div>

        <div class="reference_sec">
        <h2>Reference</h2>
          <div class="bib" style="text-align: justify">
            <p>[1] K. Grauman, et al. "Ego4d: Around the World in 3000 Hours of Egocentric Video." CVPR 2022.</p>
            <p>[2] Alexey Dosovitskiy, et al. "An Image is worth 16x16 words: Transformers  for  image recognition at scale". ICLR 2021.</p>
            <p>[3] C. Feichtenhofer, et al. "Slow-fast networks for video recognition". ICCV 2019.</p>
            <p>[4] X. Wang, et al. "Non-local neural networks". CVPR 2018.</p>
            <p>[5] J.-M. Perez-Rua, et al. "Knowing what, where and when to look: Efficient video action modeling with attention". arXiv 2020.</p>
            <p>[6] F. Sener, et al. "Temporal aggregate representations for long term video understanding". arXiv 2020.</p>
            <p>[7] X. Wang, et al. "Symbiotic attention with privileged information for egocentric action recognition". AAAI 2020.</p>
            <p>[8] A. Vaswani, et al. "Attention is all you need". arXiv 2017.</p>
            <p>[9] J. Carreira, et al. "Quo vadis, action recognition? a new model and the kinetics dataset". CVPR 2017.</p>
            <p>[10] K. He, et al. "Deep residual learning for image recognition". CVPR 2016.</p>
            <p>[11] S. Hochreiter, et al. "Long short-term memory". Neural Computation 1997.</p>
            <p>[12] Petr Kellnhofer, et al. "Gaze360: Physically Unconstrained Gaze Estimation in the Wild". ICCV 2019</p>
          </div>
        </div>

        <br></br>

</div>
</div>
</body>
</html>
